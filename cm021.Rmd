---
title: "CONJ620: CM 2.1"
subtitle: "Simple linear regression"
author: "Alison Hill"
date: "7/17/2018"
output: 
  html_document:
    toc: true
    toc_float: true
    number_sections: true
---


```{r setup, include=FALSE}
# leave this chunk alone
options(knitr.table.format = "html") 
knitr::opts_chunk$set(warning = FALSE, message = FALSE, 
  comment = NA, dpi = 300, include = FALSE)
```


# Overview

- A complete knitted `html` file is due on Sakai by end of class Tuesday July 17th (before 5pm). 
- This lab is estimated to take approximately 45 minutes. We'll work together in chunks to keep tabs on time, with the aim that we all *finish* during the in-class work period. 
- This lab is based on [Chapter 6: Basic Regression in ModernDive](http://moderndive.netlify.com/6-regression.html). Please open it and follow closely!
- You'll need to load these packages to do the lab (make sure they are installed first, not in your .Rmd file!):
```{r load_packages, include = TRUE}
library(moderndive)
library(tidyverse)
library(skimr)
```

# The data

Source: John Clay (1856). "On the Relation Between Crime, Popular
Instruction, Attendance on Religious Worship, and Beer-Houses",
Journal of the Statistical Society of London, Vol. 20 #1, pp 22-32.

In 1856, the Reverend John Clay felt that it was high time to figure out what societal factors were playing a role in the incidence of criminal behavior in Britain. He stated that:

> "It is a mere truism to say that the progress of popular education, and the formation of religious habits, are fatally opposed by the temptations to animal pleasures, which abound wherever BEER-HOUSES and low ALE-HOUSES abound."

![](http://4.bp.blogspot.com/-qdzqtghTzmU/UD_NNT9C46I/AAAAAAAABoQ/XycEe4wixVM/s1600/03.LesMiserables.US.MasteroftheHouse.jpg)

<br>
<br>
<br>

Clearly, the reverend considered public houses in Britain to be a scourge on society, namely that they "promote drunkenness and its consequent evil" (i.e., crime). Let's investigate how well we can predict criminals (per 100k population) from the number of public houses (ale/beer houses per 100k population) using simple linear regression. Here is how to read in the data (please copy and paste into a code chunk in your .Rmd):



```{r crime_data, include = TRUE}
crimenames <- c("county", "region_name", "region_code",
               "criminals", "public_houses", "school_attendance",
               "worship_attendance")

crime <- read_table("http://users.stat.ufl.edu/~winner/data/beerhall.dat",
                    col_names = crimenames)
```



# Basics (⏰ 10 min)

Note: you don't need to use R to answer these questions, but please create a section header using markdown format (`# Basics`) and type your answers there.

- What is the dependent variable?
- What is the independent variable?
- Copy and paste the provided equation that starts/ends with `$$` into your narrative (not an R code chunk), and replace `y` and `x` in this formula with meaningful variable names (you may wish to reference the `crimenames` object we made above):
`$$\hat{y} = b_0 + b_1{x}$$`
$$\hat{y} = b_0 + b_1{x}$$
- The “best-fitting” regression line is “best” in that it minimizes what?
- Why is this method called "simple linear regression" (as opposed to the method in [Chapter 7](http://moderndive.netlify.com/7-multiple-regression.html)?


# EDA

Conduct a new exploratory data analysis, which involves three things:

- Looking at the raw values.
- Computing summary statistics of the variables of interest.
- Creating informative visualizations.

## Look at the data (all together)

Use `dplyr` to figure out how many counties are in this dataset, and which variable names map onto the independent and dependent variables you identified above. 

```{r}
glimpse(crime)
```


## Look at summary statistics (⏰ 5 min)

Use `select` to select only the independent and dependent variables you identified above, then pipe those variables to the `skim` function from the `skimr` package (you should have loaded this package at the top) to see summary statistics for each. Use `dplyr::summarize` to calculate the correlation coefficient. 

```{r}
crime %>% 
  select(criminals, public_houses) %>% 
  skim()
```


```{r}
crime %>% 
  summarize(corr = cor(criminals, public_houses))
```

## Visualize the data (⏰ 5 min)


Recreate the scatterplot below of ale/beer houses per 100K on the x-axis and criminals per 100K population on the y-axis. What can you say about the relationship between public houses and criminals based on this exploration?



```{r crime_scatterplot}
ggplot(crime, aes(x = public_houses, y = criminals)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

![](`r knitr::fig_chunk('crime_scatterplot','png')`)

# Simple linear regression (⏰ 10 min)

**Part 1:** First “fit” the linear regression model to the data using the `lm()` function, then apply the `get_regression_table()` function from the `moderndive` R package to the model object. Use the output to fill in this formula with `y`, `x`, and the intercept and slope coefficients: (copy and paste into your narative: `$$\hat{y} = b_0 + b_1{x}$$`)

$$\hat{y} = b_0 + b_1{x}$$

**Part 2:** Interpret the intercept coefficient and the slope coefficient (see [here](http://moderndive.netlify.com/6-regression.html#model1table) for an example). How do the regression results match up with the results from your exploratory data analysis above?

```{r}
crime_lm <- lm(criminals ~ public_houses, data = crime)
get_regression_table(crime_lm)
```

# Observed/fitted values and residuals (⏰ 10 min)

**Part 1:** What are the observed and fitted values for the Cornwall (`ID` = 20) and Monmouth regions (`ID` = 23)? Which region do you think the reverend called "the happiest example of the infrequency of crime"?

```{r}
regression_points <- get_regression_points(crime_lm)
regression_points %>% 
  filter(ID %in% c(20, 23))
```

**Part 2:** In fact, we could argue with the reverend about the happiest example of the infrequency of crime. There are two ways you could define this, and you'll do with with a `filter` using the `|` operator (think: `or`). 

The first way is that the county had the lowest criminals overall. The second is that the county had the lowest criminals *given their number of public houses*. This would mean that the region has the lowest observed criminals, compared to the fitted value based on predicting criminals from public houses. You'll need to match the `ID` column as a row in the original data (you can just do this visually by comparing the tibbles and typing your answers in your narrative- you don't have to do a join here).

```{r}
# Derby is lowest residual (ID = 20)
# Cornwall is the lowest observed (ID = 33)
regression_points %>% 
  filter(criminals == min(criminals) | residual == min(residual))

regression_points %>% 
  left_join(select(crime, county, criminals, public_houses)) %>% 
  filter(criminals == min(criminals) | residual == min(residual))
```

# Residual analysis (⏰ 10 min)

**Part 1:** Perform a residual analysis and look for any systematic patterns in the residuals. Ideally, there should be little to no pattern- why? Does it seem like there is no systematic pattern to the residuals? 
  
```{r crime_resid_scatter}
ggplot(regression_points, aes(x = public_houses, y = residual)) +
  geom_point() +
  labs(x = "Public Houses", y = "Residual") +
  geom_hline(yintercept = 0, col = "blue", size = 1)
```

![](`r knitr::fig_chunk('crime_resid_scatter','png')`)

**Part 2:** Recreate this density plot of the residuals (hint: `?geom_vline()`). Recall that we would like the residuals to be normally distributed with mean 0. Use `dplyr` to calculate the mean of the residuals- is it (pretty close to) 0? Do you think you have more positive residuals than negative, or vice versa?

```{r crime_resid_density}
ggplot(regression_points, aes(x = residual)) +
  geom_density() +
  labs(x = "Residual") +
  geom_vline(aes(xintercept = 0), color = "red")
```

![](`r knitr::fig_chunk('crime_resid_density','png')`)

```{r}
regression_points %>% 
  summarize(mean_resid = mean(residual))
```

